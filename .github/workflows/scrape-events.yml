name: AI Event Scraper

# Schedule: Run once daily
on:
  schedule:
    # Runs at 06:00 UTC daily (1am EST / 10pm PST)
    - cron: '0 6 * * *'
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      force_run:
        description: 'Force run even if recently executed'
        required: false
        default: 'false'

# Environment variables (use GitHub Secrets for sensitive values)
env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd automation
          pip install -r requirements.txt

      - name: Create credentials file
        run: |
          cd automation
          echo '${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}' > google-credentials.json

      - name: Run AI Event Aggregator
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SHEETS_CREDENTIALS: './google-credentials.json'
          SCRAPE_DAYS_AHEAD: 90
          MIN_QUALITY_SCORE: 50
          AUTO_APPROVE_THRESHOLD: 80
          NORWICH_RADIUS_KM: 15
        run: |
          cd automation
          python ai-event-aggregator.py

      - name: Save events to JSON (optional backup)
        run: |
          # If aggregator creates output file, commit it
          if [ -f "automation/events_output.json" ]; then
            cp automation/events_output.json data/latest-scraped-events.json
            git config user.name "GitHub Actions Bot"
            git config user.email "actions@github.com"
            git add data/latest-scraped-events.json
            git diff --quiet && git diff --staged --quiet || git commit -m "ðŸ¤– Auto-update: Scraped events $(date +'%Y-%m-%d %H:%M UTC')"
            # Uncomment to push back to repo (optional):
            # git push
          fi

      - name: Cleanup credentials
        if: always()
        run: |
          rm -f automation/google-credentials.json

      - name: Summary
        run: |
          echo "âœ… Event scraping complete!"
          echo "Events have been uploaded to Google Sheets"
          echo "Google Sheet ID: ${{ secrets.GOOGLE_SHEET_ID }}"
