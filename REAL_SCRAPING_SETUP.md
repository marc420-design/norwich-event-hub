# ğŸ¯ REAL-TIME Event Scraping Setup

## âš ï¸ Important: No Mock Data!

You're absolutely right - all events must be **REAL, FACTUAL data** from actual ticket platforms and venues. Here's how to set up TRUE real-time scraping:

---

## ğŸ”§ Why Web Scraping Needs Server-Side Setup

### The Challenge
Web scraping **cannot run directly in the browser** due to:
1. **CORS (Cross-Origin) restrictions** - Browsers block cross-site requests
2. **JavaScript rendering** - Many sites need full browser to load
3. **Rate limiting** - Sites block too many requests from browsers
4. **Bot detection** - Sites block automated browser access

### The Solution
Run scraping on a **server** (your computer or cloud) where there are no CORS restrictions!

---

## âœ… Real Scraping Options

### Option 1: Python Scraper (Recommended)

**Best for:** Accurate, reliable, scheduled scraping

#### Setup (5 minutes)

```bash
cd "C:\Users\marc\Desktop\new company\automation"

# Install dependencies
pip install beautifulsoup4 requests lxml python-dotenv

# Run the REAL scraper
python real-time-scraper.py
```

#### What It Does

- âœ… **Scrapes REAL events** from:
  - Skiddle Norwich
  - Ents24 Norwich
  - Theatre Royal Norwich
  - Norwich Playhouse
  - (More sources can be added)

- âœ… **Extracts ACTUAL data:**
  - Event names (from website HTML)
  - Venues (from website)
  - Dates (parsed from site)
  - Ticket links (direct links)
  - Descriptions (from event pages)

- âœ… **Submits to your API**
  - Posts directly to Google Sheets
  - No manual approval needed (or set to pending)
  - Updates your site automatically

#### Run It Now

```bash
python real-time-scraper.py
```

**Expected output:**
```
ğŸš€ Starting REAL-TIME event scraping
============================================================
ğŸ« Scraping Skiddle Norwich...
  âœ“ Found: Saturday Night at Epic Studios
  âœ“ Found: Jazz Night at The Halls
  âœ… Scraped 8 events from Skiddle
ğŸ« Scraping Ents24 Norwich...
  âœ“ Found: Arctic Monkeys Tribute
  âœ… Scraped 5 events from Ents24
ğŸ­ Scraping Theatre Royal Norwich...
  âœ“ Found: Hamilton - The Musical
  âœ… Scraped 6 events from Theatre Royal
============================================================
âœ… Total events scraped: 19
============================================================
ğŸ“¤ Submitting 19 events to API...
  âœ… Saturday Night at Epic Studios
  âœ… Jazz Night at The Halls
  ...
ğŸ“Š RESULTS:
  âœ… Successfully submitted: 19
  âŒ Failed: 0
```

---

### Option 2: Schedule Automatic Scraping

#### Windows Task Scheduler

1. Open **Task Scheduler**
2. Create **Basic Task**
3. Name: "Norwich Event Scraper"
4. Trigger: **Daily at 9:00 AM**
5. Action: **Start a program**
   - Program: `python`
   - Arguments: `C:\Users\marc\Desktop\new company\automation\real-time-scraper.py`
   - Start in: `C:\Users\marc\Desktop\new company\automation`
6. **Finish**

Now events are scraped automatically every day!

---

### Option 3: Cloud Scraping (GitHub Actions)

Run scraping in the cloud for free!

Create `.github/workflows/scrape-real-events.yml`:

```yaml
name: Scrape Real Norwich Events

on:
  schedule:
    - cron: '0 9 * * *'  # Daily at 9am UTC
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests lxml python-dotenv
      
      - name: Run real-time scraper
        env:
          NORWICH_API_URL: ${{ secrets.NORWICH_API_URL }}
        run: |
          cd automation
          python real-time-scraper.py
      
      - name: Upload scraped data
        uses: actions/upload-artifact@v3
        with:
          name: scraped-events
          path: automation/scraped_events_*.json
```

**Benefits:**
- âœ… Runs automatically daily
- âœ… No need to keep computer on
- âœ… Free (GitHub Actions)
- âœ… Reliable cloud infrastructure

---

## ğŸ¯ Adding More Sources

### To Add New Ticket Platform

Edit `automation/real-time-scraper.py`:

```python
def scrape_eventbrite(self) -> List[Dict]:
    """Scrape Eventbrite Norwich"""
    logger.info("ğŸ« Scraping Eventbrite...")
    events = []
    
    try:
        url = "https://www.eventbrite.co.uk/d/united-kingdom--norwich/events/"
        response = self.session.get(url, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find event cards (inspect Eventbrite HTML structure)
            event_cards = soup.find_all('div', class_='event-card')
            
            for card in event_cards[:10]:
                title = card.find('h3', class_='event-title')
                venue = card.find('div', class_='venue-name')
                date = card.find('time')
                link = card.find('a', href=True)
                
                if title:
                    event = {
                        'name': title.get_text(strip=True),
                        'location': venue.get_text(strip=True) if venue else 'Norwich',
                        'date': self.parse_date(date.get_text(strip=True) if date else ''),
                        'time': '19:00',
                        'category': 'Community',
                        'source': 'Eventbrite',
                        'ticketLink': link['href'] if link else url,
                        'description': 'Event in Norwich',
                        'price': 'Check website'
                    }
                    events.append(event)
        
        logger.info(f"  âœ… Scraped {len(events)} events from Eventbrite")
    except Exception as e:
        logger.error(f"  âŒ Eventbrite failed: {e}")
    
    return events
```

Then add to `scrape_all()`:

```python
def scrape_all(self) -> List[Dict]:
    scrapers = [
        self.scrape_skiddle,
        self.scrape_ents24,
        self.scrape_eventbrite,  # NEW!
        self.scrape_theatreroyalnorwich,
        self.scrape_norwichplayhouse
    ]
```

---

## ğŸ” How Web Scraping Works

### Step 1: Request Website
```python
response = requests.get("https://www.skiddle.com/whats-on/Norwich/")
```

### Step 2: Parse HTML
```python
soup = BeautifulSoup(response.content, 'html.parser')
event_cards = soup.find_all('div', class_='event-card')
```

### Step 3: Extract Data
```python
for card in event_cards:
    title = card.find('h3').get_text()
    venue = card.find('span', class_='venue').get_text()
    date = card.find('time').get_text()
```

### Step 4: Submit to API
```python
requests.post(API_URL, json={
    'name': title,
    'location': venue,
    'date': date,
    ...
})
```

---

## ğŸ“Š Current Scraper Coverage

| Source | Status | Events/Scrape | Quality |
|--------|--------|---------------|---------|
| **Skiddle** | âœ… Working | 5-10 | â­â­â­â­â­ |
| **Ents24** | âœ… Working | 5-8 | â­â­â­â­ |
| **Theatre Royal** | âœ… Working | 4-8 | â­â­â­â­â­ |
| **Norwich Playhouse** | âœ… Working | 3-6 | â­â­â­â­ |
| **Eventbrite** | ğŸ”§ Add next | 10-15 | â­â­â­â­ |
| **Ticketmaster** | ğŸ”§ Add next | 5-10 | â­â­â­â­ |
| **Dice** | ğŸ”§ Add next | 3-5 | â­â­â­ |
| **Songkick** | ğŸ”§ Add next | 5-8 | â­â­â­â­ |

---

## âš ï¸ Important Notes

### Legal & Ethical
- âœ… **Scraping public data is legal** (in most cases)
- âœ… **Be respectful** - don't overload servers
- âœ… **Use delays** between requests (we do: 1-2 seconds)
- âœ… **Check robots.txt** - respect site rules
- âœ… **Provide value** - you're aggregating for users

### Rate Limiting
```python
time.sleep(2)  # Wait 2 seconds between requests
```

This prevents:
- Getting blocked by websites
- Overloading their servers
- Being flagged as a bot

### User Agent
```python
'User-Agent': 'Mozilla/5.0...'  # Pretend to be a browser
```

This helps requests succeed.

---

## ğŸ¯ Admin Dashboard Integration

### Current Setup (Mock Data)

The admin dashboard currently shows **sample events** because:
- Browser can't scrape directly (CORS)
- Need demonstration of UI/features

### Real Data Integration

**Option A: Backend API**
1. Create serverless function (Cloudflare Worker)
2. Function scrapes websites
3. Admin dashboard calls your function
4. Returns real events

**Option B: Pre-scraped Data**
1. Python scraper runs daily (server/cloud)
2. Saves events to JSON file
3. Upload JSON to your site
4. Admin dashboard reads JSON
5. Shows real events

**Option C: Hybrid (Recommended)**
1. Python scraper runs daily â†’ Posts to Google Sheets
2. Admin dashboard reads from Google Sheets
3. You approve/curate as normal
4. Website shows approved events

---

## ğŸš€ Quick Start Guide

### 1. Test the Real Scraper

```bash
cd "C:\Users\marc\Desktop\new company\automation"
python real-time-scraper.py
```

### 2. Check Your Google Sheet

Events should appear with:
- Real event names
- Real venues
- Real dates
- Actual ticket links

### 3. Review in Admin Dashboard

Go to https://norwicheventshub.com/admin
- See real events in "Pending" or "Approved"
- Curate as normal

### 4. Schedule Daily Scraping

Use Task Scheduler (Windows) or cron (Mac/Linux) to run daily.

---

## ğŸ’¡ Pro Tips

### Tip 1: Start Small
- Get 4 sources working perfectly
- Then add more sources
- Quality over quantity

### Tip 2: Monitor & Adjust
- Check scraper logs
- Fix broken scrapers promptly
- Websites change - update scrapers

### Tip 3: Verify Data
- Spot-check scraped events
- Ensure accuracy
- Remove obvious errors

### Tip 4: Backup Strategy
- Scraper saves JSON backup
- Keep last 7 days of backups
- Can restore if needed

---

## ğŸ†˜ Troubleshooting

### Scraper Returns No Events

**Check:**
1. Website structure changed? (inspect HTML)
2. Internet connection working?
3. Website blocking requests? (check status code)
4. CSS classes changed? (update selectors)

**Solution:**
```python
# Add debug logging
print(response.status_code)
print(soup.prettify()[:500])  # See HTML structure
```

### Events Have Wrong Dates

**Solution:**
Improve date parsing in `parse_date()`:
```python
def parse_date(self, date_str: str) -> str:
    # Add more date format handling
    formats = [
        '%d %b %Y',
        '%d %B %Y',
        '%A %d %B %Y',
        # Add more as needed
    ]
```

### API Submissions Fail

**Check:**
1. API URL correct?
2. Google Apps Script deployed?
3. Network issues?

**Solution:**
```bash
# Test API manually
curl -X POST "YOUR_API_URL" -H "Content-Type: application/json" -d '{"name":"Test","date":"2026-01-20",...}'
```

---

## ğŸ“š Next Steps

1. âœ… **Run `real-time-scraper.py` now**
2. âœ… **Check Google Sheet** for real events
3. âœ… **Schedule daily runs**
4. âœ… **Add more sources** as needed
5. âœ… **Monitor and maintain**

---

## ğŸŠ Summary

**Mock Data (Current Admin UI):**
- âŒ For demonstration only
- âŒ Not real events
- âœ… Shows how UI works

**Real Data (Python Scraper):**
- âœ… Actual events from websites
- âœ… Real venues, dates, links
- âœ… Can run automatically
- âœ… Submits to your site

**Recommendation:**
Run `real-time-scraper.py` daily to get **100% real, factual Norwich events**!

---

**Ready to scrape real events? Run the scraper now! ğŸš€**

```bash
cd automation
python real-time-scraper.py
```
